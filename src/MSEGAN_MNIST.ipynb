{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","background_execution":"on","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["# connect with google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UC7Y1zSV1zUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! nvidia-smi \n","! nvidia-smi -L"],"metadata":{"id":"kk-YhR3C12sz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras.layers import Input\n","\n","#accuracy metric\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import RandomForestClassifier\n","\n","def accuracy(train_x, train_y, test_x, test_y):\n","    cart_model = RandomForestClassifier(50)\n","    cart_model.fit(train_x, train_y)\n","    labels_pred = cart_model.predict(test_x)\n","    current_acc = accuracy_score(test_y, labels_pred) * 100\n","    return current_acc\n"],"metadata":{"id":"hDF42dpf13Q7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#set random seed\n","#GPU 0: Tesla V100-SXM2-16GB\n","import os\n","import random\n","import tensorflow as tf\n","import numpy as np\n","\n","#check version\n","print('numpy',np.__version__)\n","print('tensorflow',tf.__version__)\n","!python --version\n","def set_seed(seed=784):\n","\n","  os.environ['PYTHONHASHSEED']=str(0)\n","  random.seed(seed)\n","  tf.random.set_seed(seed)\n","  tf.keras.utils.set_random_seed(seed)\n","  tf.compat.v1.set_random_seed(seed)\n","  np.random.seed(seed)\n","  os.environ['TF_DETERMINISTIC_OPS'] = '0' \n","\n","seed='5'\n","set_seed(int(seed))\n","print(int(seed))\n","\n","#seed:5,10,20,31,40,50,60,70,104,567,\n","      # 784,871,1002,1945,4000,3912,5678,40532,78,90\n","      # 43,56,563,25,34,64,66,18,34,27\n"],"metadata":{"id":"yl4d9Soa17p7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","import tensorflow as tf\n","import os\n","import numpy as np\n","\n","MNIST_PATH = \"./mnist.npz\"\n","def load_mnist(path):\n","    if os.path.isfile(path):\n","        with np.load(path, allow_pickle=True) as f:\n","            x_train, y_train = f['x_train'], f['y_train']\n","            x_test, y_test = f['x_test'], f['y_test']\n","        return (x_train, y_train), (x_test, y_test)\n","    return keras.datasets.mnist.load_data(MNIST_PATH)\n","\n","def get_half_batch_ds(batch_size):\n","    return get_ds(batch_size//2)\n","\n","def get_ds(batch_size):\n","    (x, y), (x_T,y_T) = load_mnist(MNIST_PATH)\n","    x = _process_x(x)\n","    x_T=_process_x(x_T)\n","\n","    y = tf.cast(y, tf.int32)\n","    y_T = tf.cast(y_T, tf.int32)\n","\n","    ds = tf.data.Dataset.from_tensor_slices((x, y)).cache().shuffle(1024).batch(batch_size) \\\n","        .prefetch(tf.data.experimental.AUTOTUNE)\n","    #.cache 将preprocess的数据存储在缓存空间中将大幅提高计算速度\n","    #prefetch 提速\n","    return ds,x_T.numpy(),y_T.numpy()\n","def _process_x(x):\n","    return tf.expand_dims(tf.cast(x, tf.float32), axis=3) / 255. * 2 - 1"],"metadata":{"id":"k1GkSh2uM5aV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MSE GAN"],"metadata":{"id":"PZ1atZ_mOOkg"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","from PIL import Image\n","\n","def _img_recenter(img):\n","    return (img + 1) * 255 / 2\n","\n","def _save_gan(model_name, ep, t,imgs, seed,show_label=False, nc=10, nr=10):\n","    if not isinstance(imgs, np.ndarray):\n","        imgs = imgs.numpy()\n","    if imgs.ndim > 3:\n","        imgs = np.squeeze(imgs, axis=-1)\n","    imgs = _img_recenter(imgs)\n","    plt.clf()\n","    plt.figure(0, (nc * 2, nr * 2))\n","    for c in range(nc):\n","        for r in range(nr):\n","            i = r * nc + c\n","            plt.subplot(nr, nc, i + 1)\n","            plt.imshow(imgs[i], cmap=\"gray\")\n","            plt.axis(\"off\")\n","            if show_label:\n","                plt.text(23, 26, int(r), fontsize=23)\n","    plt.tight_layout()\n","    dir_ = \"drive/My Drive/VGAN/\"+seed+\"/visual_mse_\"+seed\n","    os.makedirs(dir_, exist_ok=True)\n","    path = dir_ + \"/\"+str(ep)+\"_\"+str(t)+\".png\"\n","    plt.savefig(path)\n","\n","def save_gan(model, ep,t,seed, **kwargs):\n","    name = model.__class__.__name__.lower()\n","    if name in [\"dcgan\", \"wgan\", \"wgangp\", \"lsgan\", \"wgandiv\", \"sagan\", \"pggan\"]:\n","        imgs = model.call(100, training=False).numpy()\n","        _save_gan(name, ep, imgs, show_label=False)\n","    elif name == \"gan\":\n","        data = model.call(5, training=False).numpy()\n","        plt.plot(data.T)\n","        plt.xticks((), ())\n","        dir_ = \"visual/{}\".format(name)\n","        os.makedirs(dir_, exist_ok=True)\n","        path = dir_ + \"/{}.png\".format(ep)\n","        plt.savefig(path)\n","    elif name == \"cgan\" or name == \"acgan\":\n","        img_label = np.arange(0, 10).astype(np.int32).repeat(10, axis=0)\n","        imgs = model.predict(img_label)\n","        _save_gan(name, ep, t,imgs, seed,show_label=True)\n","\n","    else:\n","        raise ValueError(name)\n","    plt.clf()\n","    plt.close()\n","\n","\n","\n","import tensorflow as tf\n","import os\n","\n","_b_acc = None\n","_c_acc = None\n","\n","def set_soft_gpu(soft_gpu):\n","    if soft_gpu:\n","        gpus = tf.config.experimental.list_physical_devices('GPU')\n","        if gpus:\n","            # Currently, memory growth needs to be the same across GPUs\n","            for gpu in gpus:\n","                tf.config.experimental.set_memory_growth(gpu, True)\n","            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","def binary_accuracy(label, pred):\n","    global _b_acc\n","    if _b_acc is None:\n","        _b_acc = tf.keras.metrics.BinaryAccuracy()\n","    _b_acc.reset_states()\n","    _b_acc.update_state(label, pred)\n","    return _b_acc.result()\n","\n","def save_weights(model,ep,t,seed):\n","    os.makedirs(\"drive/My Drive/VGAN/\"+seed+\"/Model_mse_\"+seed+\"/\", exist_ok=True)\n","    model.save_weights(\"drive/My Drive/VGAN/\"+seed+\"/Model_mse_\"+seed+\"/Model_\"+str(ep)+\"_\"+str(t)+\"/Model.ckpt\")\n","\n","#define V-method\n","from numba import jit\n","\n","@jit()\n","def Vmatrix_np(x):\n","    v_matrix=np.zeros((x.shape[0],x.shape[0]))\n","    ve=np.zeros((1,x.shape[1]))\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[0]):\n","            for m in range(x.shape[1]):\n","                xt = x[:, m]\n","                ve[0,m] = np.subtract(np.max(xt),np.maximum(xt[i], xt[j]))\n","            v=np.sum(ve)\n","            v_matrix[i,j]=v\n","             \n","    return v_matrix\n","def vloss_tf_bigan(x_r, x_f, v):\n","    dif = x_f - x_r\n","    loss = tf.matmul(tf.matmul(dif, v, transpose_a=True), dif)\n","    return loss\n","\n","\n","from tensorflow.keras.layers import Conv2D, Dropout, Flatten, Dense, Reshape, Conv2DTranspose, ReLU, BatchNormalization, LeakyReLU\n","from tensorflow import keras\n","import tensorflow as tf\n","\n","def mnist_uni_disc_cnn(input_shape=(28, 28, 1), use_bn=True):\n","    model = keras.Sequential()\n","    # [n, 28, 28, n] -> [n, 14, 14, 64]\n","    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', input_shape=input_shape))\n","    if use_bn:\n","        model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","    model.add(Dropout(0.3))\n","    # -> [n, 7, 7, 128]\n","    model.add(Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n","    if use_bn:\n","        model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","    model.add(Dropout(0.3))\n","    model.add(Flatten())\n","    return model\n","\n","def mnist_uni_gen_cnn(input_shape):\n","    return keras.Sequential([\n","        # [n, latent] -> [n, 7 * 7 * 128] -> [n, 7, 7, 128]\n","        Dense(7 * 7 * 64, input_shape=input_shape),\n","        BatchNormalization(),\n","        ReLU(),\n","        Reshape((7, 7, 64)),\n","        # -> [n, 14, 14, 64]\n","        Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same'),#channel=64,kernelsize(4,4)\n","        BatchNormalization(),\n","        ReLU(),\n","        # -> [n, 28, 28, 32]\n","        Conv2DTranspose(16, (4, 4), strides=(2, 2), padding='same'),\n","        BatchNormalization(),\n","        ReLU(),\n","        # -> [n, 28, 28, 1]\n","        Conv2D(1, (4, 4), padding='same', activation=keras.activations.tanh)\n","    ])\n","# [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784.pdf)\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","from tensorflow.keras.layers import Dense, Reshape, Input, Embedding\n","import time\n","\n","class CGAN(keras.Model):\n","    \"\"\"\n","    discriminator 标签+图片 预测 真假\n","    generator 标签 生成 图片\n","    \"\"\"\n","    def __init__(self, latent_dim, label_dim, img_shape):\n","        #全局设置\n","        super().__init__()\n","        self.latent_dim = latent_dim #设置潜在空间的维度大小\n","        self.label_dim = label_dim\n","        self.img_shape = img_shape\n","\n","        self.g = self._get_generator()\n","        self.d = self._get_discriminator()\n","\n","        self.opt = keras.optimizers.Adam(0.0002, beta_1=0.5)\n","        self.loss_func = keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","    def call(self, target_labels, training=None, mask=None):\n","        noise = tf.random.normal((len(target_labels), self.latent_dim))\n","        if isinstance(target_labels, np.ndarray):\n","            target_labels = tf.convert_to_tensor(target_labels, dtype=tf.int32)\n","        return self.g.call([noise, target_labels], training=training)\n","      \n","    def d_loss_wasserstein(self, real_logits, fake_logits):\n","        d_loss = tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n","\n","        return d_loss\n","\n","    def g_loss_wasserstein(self, fake_logits):\n","        g_loss = -tf.reduce_mean(fake_logits)\n","\n","        return g_loss\n","\n","    def wasserstein_gradient_penalty(self, x, x_fake, labels):\n","        # temp_shape = [x.shape[0]]+[1 for _ in  range(len(x.shape)-1)]\n","        epsilon = tf.random.uniform([], 0.0, 1.0)\n","        x_hat = epsilon * x + (1 - epsilon) * x_fake\n","\n","        with tf.GradientTape() as t:\n","            t.watch(x_hat)\n","            d_hat = self.d([x_hat, labels], training=False)\n","        gradients = t.gradient(d_hat, x_hat)\n","        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients)))\n","        gradient_penalty = 1 * tf.reduce_mean((slopes - 1.0) ** 2)\n","\n","        return gradient_penalty\n","\n","    def _get_discriminator(self):\n","        img = Input(shape=self.img_shape)\n","        label = Input(shape=(), dtype=tf.int32)\n","        label_emb = Embedding(10, 32)(label)\n","        emb_img = Reshape((28, 28, 1))(Dense(28*28, activation=keras.activations.relu)(label_emb))\n","        concat_img = tf.concat((img, emb_img), axis=3)\n","        s = keras.Sequential([\n","            mnist_uni_disc_cnn(input_shape=[28, 28, 2]),\n","            Dense(1)\n","        ])\n","        o = s(concat_img)\n","        model = keras.Model([img, label], o, name=\"discriminator\")\n","        model.summary()\n","        return model\n","\n","    def _get_generator(self):\n","        noise =  Input(shape=(self.latent_dim,))\n","        label = Input(shape=(), dtype=tf.int32)\n","        label_onehot = tf.one_hot(label, depth=self.label_dim)\n","        model_in = tf.concat((noise, label_onehot), axis=1)\n","        s = mnist_uni_gen_cnn((self.latent_dim+self.label_dim,))\n","        o = s(model_in)\n","        model = keras.Model([noise, label], o, name=\"generator\")\n","        model.summary()\n","        return model\n","# d_loss, d_acc = self.train_d(img, gen_img,img_label,gen_img_label)\n","    def train_d(self, img,gen_img, img_label):\n","        with tf.GradientTape() as tape:\n","            real_logits = self.d.call([img, img_label], training=True)\n","            fake_logits = self.d.call([gen_img, img_label], training=True)\n","\n","            # loss = self.loss_func(pred)\n","            loss=self.d_loss_wasserstein(real_logits,fake_logits)\n","            gp=self.wasserstein_gradient_penalty(img,gen_img,img_label)\n","            gloss=loss\n","        grads = tape.gradient(gloss, self.d.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.d.trainable_variables))\n","        return gloss\n","\n","    def train_g(self, real_img,random_img_label):\n","        d_label = tf.ones((len(random_img_label), 1), tf.float32)   # let d think generated images are real\n","        with tf.GradientTape() as tape:\n","            g_img = self.call(random_img_label, training=True)\n","            fake_logits = self.d.call([g_img, random_img_label], training=False)\n","            #wloss\n","            loss=self.g_loss_wasserstein(fake_logits)\n","            # kl loss\n","            # loss = self.loss_func(d_label, pred)\n","\n","            #mse loss\n","            loss_mse=tf.reduce_mean(tf.losses.mean_squared_error(real_img,g_img))\n","            g_loss=loss+loss_mse\n","        grads = tape.gradient(g_loss, self.g.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.g.trainable_variables))\n","        return g_loss, g_img,loss_mse\n","\n","    def step(self, real_img, real_img_label):\n","        g_loss, g_img ,loss_mse= self.train_g(real_img,real_img_label)\n","\n","        gen_img=g_img[:len(g_img)]\n","        img = real_img\n","\n","        img_label =real_img_label\n","\n","        d_loss= self.train_d(img, gen_img,img_label)\n","        return g_img, d_loss, g_loss,loss_mse\n","\n","\n","def train(gan, ds,x_T,y_T,seed):\n","    t0 = time.time()\n","\n","    loss_G=[]\n","    loss_D=[]\n","    acc_value=[]\n","\n","    acc=0\n","    for ep in range(EPOCH):\n","        for t, (real_img, real_img_label) in enumerate(ds):\n","            # print(real_img_label.shape)\n","            g_img, d_loss, g_loss,loss_mse = gan.step(real_img, real_img_label)\n","            loss_G.append(loss_G)\n","            loss_D.append(loss_D)\n","            y_t = np.arange(0, 10).astype(np.int32).repeat(32, axis=0)\n","            x_t = gan.predict(y_t)\n","\n","            x_t=x_t.reshape(-1,28*28*1)\n","            x_T=x_T.reshape(-1,28*28*1)\n","\n","            current_acc=accuracy(x_t,y_t,x_T,y_T)\n","\n","            if current_acc>acc:\n","              acc=current_acc\n","              t1 = time.time()\n","              acc_value.append([current_acc,t])\n","              print(\"ep={} | time={:.2f} | t={}  | d_loss={:.5f} | g_loss={:.5f}| mse_loss={:.5f}\".format(\n","              ep, t1-t0, t, d_loss.numpy(), g_loss.numpy(),loss_mse.numpy() ))\n","              save_gan(gan,ep, t,seed)\n","              save_weights(gan,ep,t,seed)\n","              print('The best acccuracy is:',current_acc)\n","\n","    print('asdasdad')\n","    y_t = np.arange(0, 10).astype(np.int32).repeat(32, axis=0)\n","    x_t = m.predict(y_t)\n","    x_t=x_t.reshape(-1,28*28*1)\n","    x_T=x_T.reshape(-1,28*28*1)\n","\n","    last_acc=accuracy(x_t,y_t,x_T,y_T)\n","    save_gan(gan,ep, t,seed)\n","    save_weights(gan,ep,t,seed)\n","    print('The last accuracy is:',last_acc)\n","\n","    t_end=time.time()\n","    print('total running :',t_end-t0)\n","\n","#run\n","LATENT_DIM = 100\n","IMG_SHAPE = (28, 28, 1)\n","LABEL_DIM = 10\n","BATCH_SIZE = 64\n","EPOCH = 5\n","\n","set_soft_gpu(True)\n","d,x_T,y_T = get_half_batch_ds(BATCH_SIZE)\n","m = CGAN(LATENT_DIM, LABEL_DIM, IMG_SHAPE)\n","train(m, d,x_T,y_T,seed)"],"metadata":{"id":"1w8y2aPxLdO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ep=5\n","# t=1533\n","# dir='drive/Shared drives/Ziqiang/VGAN/Model_mse/'\n","# checkpoint_dir=dir+'Model_'+str(ep)+'_'+str(t)+'/Model.ckpt'\n","# print(checkpoint_dir)\n","# m.load_weights(checkpoint_dir)\n","\n","# img_label = np.arange(0, 10).astype(np.int32).repeat(32, axis=0)\n","# imgs = m.predict(img_label)\n","\n","# print(imgs.shape)\n","# # #visual\n","# imgs = np.squeeze(imgs, axis=-1)\n","# imgs = _img_recenter(imgs)\n","# for i in range(10): \n","#   plt.figure(i)\n","#   plt.imshow(imgs[32*i+3], cmap=\"gray\")"],"metadata":{"id":"5sA0YRqujUNh"},"execution_count":null,"outputs":[]}]}
